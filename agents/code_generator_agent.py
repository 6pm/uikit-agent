"""
Agent task which run code generation using LangGraph
"""

import logging
from typing import Any

from langchain_core.messages import HumanMessage, SystemMessage

# imports for Gemini model
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import END, START, MessagesState, StateGraph

from schemas.ai_models.test_ai_response import TestAIResponse
from schemas.api.code_generation_types import CodeGenerationRequest

logger = logging.getLogger(__name__)


def mock_llm(state: MessagesState) -> TestAIResponse:
    print("state mock_llm", state)
    return {"messages": [{"role": "ai", "content": "hello world and Slava"}]}


class CodeGeneratorAgent:
    def __init__(self):
        """Initialize agent with default values."""

        self.graph = None
        self.gemini_model = None

    async def _initialize(self):
        """
        Async initialization method.
        Call this after creating the instance if you need async setup.
        """
        # Example: async initialization logic here
        # await self._setup_llm()
        # await self._setup_tools()

        await self.init_gemini_model()
        await self.build_graph()

    @classmethod
    async def create(cls):
        """
        Factory method for async initialization.

        Usage:
            agent = await CodeGeneratorAgent.create()

        Returns:
            CodeGeneratorAgent: Initialized agent instance
        """
        instance = cls()
        await instance._initialize()
        return instance

    async def init_gemini_model(self):
        """
        Initialize Gemini model
        """

        self.gemini_model = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash-lite",
            temperature=0,  # 0 –¥–ª—è –∫–æ–¥–∏–Ω–≥—É –∫—Ä–∞—â–µ (–±—ñ–ª—å—à –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–æ)
            max_retries=2,
        )

    # —Ä–µ–∞–ª—å–Ω–∞ –Ω–æ–¥–∞ —â–æ –≤–∏–∫–ª–∏–∫–∞—î Gemini –º–æ–¥–µ–ª—å
    def call_model(self, state: MessagesState):
        """
        Invokes the Gemini model with the current state messages.
        """
        messages = state["messages"]

        # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ System Message, —è–∫—â–æ –π–æ–≥–æ –Ω–µ–º–∞—î –≤ —ñ—Å—Ç–æ—Ä—ñ—ó
        # –ê–ª–µ –∫—Ä–∞—â–µ —Ü–µ —Ä–æ–±–∏—Ç–∏ –ø—Ä–∏ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—ñ –∑–∞–ø–∏—Ç—É (–¥–∏–≤. –Ω–∏–∂—á–µ)

        logger.info("ü§ñ Calling Gemini model...")
        response = self.gemini_model.invoke(messages)

        # LangGraph –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –¥–æ–¥–∞—Å—Ç—å —Ü–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–æ —ñ—Å—Ç–æ—Ä—ñ—ó (append)
        return {"messages": [response]}

    async def build_graph(self):
        """
        Generate code from Figma JSON

        Args:
            json_data: Figma JSON

        Returns:
            dict: Code generation result
        """

        # Set up Graph Builder with State
        graph_builder = StateGraph(MessagesState)

        # –î–æ–¥–∞—î–º–æ —Ä–µ–∞–ª—å–Ω—É –Ω–æ–¥—É
        # –ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É: –º–∏ –ø–µ—Ä–µ–¥–∞—î–º–æ self.call_model, –±–æ —Ü–µ –º–µ—Ç–æ–¥ –∫–ª–∞—Å—É
        graph_builder.add_node("generate_code", self.call_model)

        # –ë—É–¥—É—î–º–æ –ø—Ä–æ—Å—Ç–∏–π –ø–æ—Ç—ñ–∫
        graph_builder.add_edge(START, "generate_code")
        graph_builder.add_edge("generate_code", END)
        # end test mock code

        # ------------------------------------------------------------------------------------------------

        # # Add nodes
        # graph_builder.add_node("worker", self.worker)
        # graph_builder.add_node("tools", ToolNode(tools=self.tools))
        # graph_builder.add_node("evaluator", self.evaluator)

        # # Add edges
        # graph_builder.add_conditional_edges(
        #     "worker", self.worker_router, {"tools": "tools", "evaluator": "evaluator"}
        # )
        # graph_builder.add_edge("tools", "worker")
        # graph_builder.add_conditional_edges(
        #     "evaluator", self.route_based_on_evaluation, {"worker": "worker", "END": END}
        # )
        # graph_builder.add_edge(START, "worker")

        # # Compile the graph
        # self.graph = graph_builder.compile(checkpointer=self.memory)

        self.graph = graph_builder.compile()

    async def generate_code(self, request_data: CodeGenerationRequest) -> dict[str, Any]:
        """
        Generate code from Figma JSON data.
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –∑–∞–ø–∏—Ç –≤ JSON-—Ä—è–¥–æ–∫
        message_str = request_data.model_dump_json()

        # 5. –§–æ—Ä–º—É—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –≤—Ö—ñ–¥–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        # –î–æ–¥–∞—î–º–æ SystemMessage, —â–æ–± –∑–∞–¥–∞—Ç–∏ —Ä–æ–ª—å –º–æ–¥–µ–ª—ñ
        system_prompt = """You are an expert Frontend Developer.
Your task is to generate clean, production-ready code based on the provided Figma JSON data.
Do not include conversational filler, output only the code or JSON result."""

        inputs = {"messages": [SystemMessage(content=system_prompt), HumanMessage(content=message_str)]}

        # –í–∏–∫–ª–∏–∫–∞—î–º–æ –≥—Ä–∞—Ñ
        result = await self.graph.ainvoke(inputs)

        # result['messages'][-1] –±—É–¥–µ –æ—Å—Ç–∞–Ω–Ω—å–æ—é –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é –≤—ñ–¥ AI
        last_message = result["messages"][-1]

        return {
            "content": last_message.content,
            # "usage": last_message.usage_metadata # –Ø–∫—â–æ —Ç—Ä–µ–±–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω—ñ–≤
        }
